{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RAPPOR â€“ Server-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disabling scientific notation makes it a bit easier to compare numeric values with the ones logged in the R part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data files\n",
    "\n",
    "In production, the content of these files is coming from the clients. For testing, we just use files that are generated by Google's simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = \"../rappor/\"\n",
    "base += \"_tmp/python/r-gauss-small-sim_bloom_filter1_1/\"\n",
    "\n",
    "PARAMS_PATH = base + \"case_params.csv\"\n",
    "COUNTS_PATH = base + \"1/case_counts.csv\"\n",
    "MAP_PATH = base + \"case_map.csv\"\n",
    "CANDIDATES_PATH = base + \"case_unique_values.txt\" #\"case_candidates.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k, h, m, p, q, f = pd.read_csv(PARAMS_PATH).iloc[0]\n",
    "k = int(k)\n",
    "h = int(h)\n",
    "m = int(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The Bloom filter has k=4 bits and uses h=2 hash functions\n",
      "- There are m=32 cohorts\n",
      "- There is a f=0.00 probability of randomly changing a bit for the PRR\n",
      "- There is a p=0.50 (q=0.75) probability of setting a bit in the IRR to 1 if that bit was 0 in the PRR\n"
     ]
    }
   ],
   "source": [
    "print \"- The Bloom filter has k=%d bits and uses h=%d hash functions\" % (k, h)\n",
    "print \"- There are m=%d cohorts\" % m\n",
    "print \"- There is a f=%.2f probability of randomly changing a bit for the PRR\" % f\n",
    "print \"- There is a p=%.2f (q=%.2f) probability of setting a bit in the IRR to 1 if that bit was 0 in the PRR\" % (p, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts\n",
    "\n",
    "Each row contains information about one cohort. The very first value in a row specifies the total number of reports in that cohort. The other values specify how often the respective bit was set in the sent Bloom Filter. Because some bits were randomly changed, the first value can be smaller or greater than the sum of the other values in that row.\n",
    "\n",
    "Cohorts without any reports are directly removed.\n",
    "\n",
    "In the real server implementation, we will need a little bit of additional logic for calculating these sums. Here, we just use what Google's implementation already provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31250</td>\n",
       "      <td>18176</td>\n",
       "      <td>19389</td>\n",
       "      <td>18710</td>\n",
       "      <td>19497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31250</td>\n",
       "      <td>18527</td>\n",
       "      <td>19047</td>\n",
       "      <td>19137</td>\n",
       "      <td>18602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31250</td>\n",
       "      <td>18500</td>\n",
       "      <td>18459</td>\n",
       "      <td>19104</td>\n",
       "      <td>19343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31250</td>\n",
       "      <td>17751</td>\n",
       "      <td>19476</td>\n",
       "      <td>18634</td>\n",
       "      <td>19038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31250</td>\n",
       "      <td>18890</td>\n",
       "      <td>19827</td>\n",
       "      <td>19512</td>\n",
       "      <td>18683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1      2      3      4\n",
       "0  31250  18176  19389  18710  19497\n",
       "1  31250  18527  19047  19137  18602\n",
       "2  31250  18500  18459  19104  19343\n",
       "3  31250  17751  19476  18634  19038\n",
       "4  31250  18890  19827  19512  18683"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = DataFrame.from_csv(COUNTS_PATH, header=None, index_col=None)\n",
    "counts = counts[counts[0] != 0]\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map (hash values)\n",
    "\n",
    "One row for every candidate string. The leftmost value in a row (Pandas' index) shows the respective string, the other values (number of cohorts * number of hash functions) show the hashed values of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>v1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>109</td>\n",
       "      <td>114</td>\n",
       "      <td>115</td>\n",
       "      <td>119</td>\n",
       "      <td>120</td>\n",
       "      <td>123</td>\n",
       "      <td>121</td>\n",
       "      <td>127</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "      <td>114</td>\n",
       "      <td>120</td>\n",
       "      <td>117</td>\n",
       "      <td>122</td>\n",
       "      <td>122</td>\n",
       "      <td>128</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v3</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>120</td>\n",
       "      <td>119</td>\n",
       "      <td>122</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>113</td>\n",
       "      <td>120</td>\n",
       "      <td>117</td>\n",
       "      <td>123</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>111</td>\n",
       "      <td>116</td>\n",
       "      <td>114</td>\n",
       "      <td>120</td>\n",
       "      <td>118</td>\n",
       "      <td>121</td>\n",
       "      <td>122</td>\n",
       "      <td>126</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    1   2   3   4   5   6   7   8   9   10 ...    55   56   57   58   59   60  \\\n",
       "0                                          ...                                  \n",
       "v1   2   4   6   7  12   9  15  15  20  17 ...   112  109  114  115  119  120   \n",
       "v2   3   4   8   7   9  10  16  16  20  18 ...   110  110  113  114  120  117   \n",
       "v3   2   1   6   5  10  12  16  16  19  19 ...   110  109  113  115  120  119   \n",
       "v4   2   1   8   6  10  11  14  14  20  19 ...   112  110  114  113  120  117   \n",
       "v5   1   3   5   7  12  11  16  13  19  19 ...   112  111  116  114  120  118   \n",
       "\n",
       "     61   62   63   64  \n",
       "0                       \n",
       "v1  123  121  127  126  \n",
       "v2  122  122  128  126  \n",
       "v3  122  123  128  127  \n",
       "v4  123  123  128  127  \n",
       "v5  121  122  126  127  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maps = DataFrame.from_csv(MAP_PATH, header=None)\n",
    "maps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### candidates\n",
    "\n",
    "Here, the candidates are simply the values that were also used by the client-side code. Generally, we would like to use a superset of these values, e.g. the most popular websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidates():\n",
    "    with open(CANDIDATES_PATH) as f:\n",
    "        lines = f.readlines()\n",
    "        return [candidate.strip() for candidate in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = get_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for testing, we'll try a superset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = [\"v%d\" % i for i in range(300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v0', 'v1', 'v2', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data to separate signal and noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high-level task of the server-side is to find out which candidate strings were really used by the clients. We will use statistical techniques for doing this, namely linear regression to find out which candidate strings probably influenced the final result.\n",
    "\n",
    "Another way to think about this: In Machine Learning and statistics we are often concerned with separating signal from noise. Here, the signal is given by the hashed values of candidate strings. The noise is added on purpose by clients in order to maintain privacy. On the server-side, we then try to use statistical techniques to remove the noise from the aggregated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target values $y$: Estimating true counts of the original Bloom filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we do some basic preprocessing to be able to use the same variables that are also being used by the RAPPOR paper. This makes it a bit easier to implement the math formulas given in the paper.\n",
    "\n",
    "$N$ is a vector containing the number of reports from the individual cohorts. $c$ is a matrix\n",
    "where $c_{ij}$ tells us how often bit $j$ was set in cohort $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = counts[0].as_matrix()\n",
    "c = counts.drop([0], axis=1).as_matrix().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target values $y$ will contain estimates of how often the individual bits were really set in the original bloom filter.\n",
    "\n",
    "$$\n",
    "t_{ij} = \\frac{c_{ij} - (p + 0.5fq - 0.5fp) N_j}{(1 - f) (q - p)}\n",
    "$$\n",
    "\n",
    "$Y$ is then simply a long vector that contains the rows of $t$ flattened. `estimate_bloom_counts` calculates $y$ in a vectorized way.\n",
    "\n",
    "**TODO**: Understand why this formula holds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_bloom_count(c, N):\n",
    "    Y = c - ((p + 0.5 * f * q - 0.5 * f * p) * N)\n",
    "    Y /= ((1 - f) * (q - p))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y = estimate_bloom_count(c, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 32)\n"
     ]
    }
   ],
   "source": [
    "print Y.shape\n",
    "assert(Y.shape == (k, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not mentioned in the paper, but actually $Y$ is then also divided by $N$ to get frequencies instead of counts. We also reshape the matrix to one long vector, and call this $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = (Y / N).T.reshape(k * m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix $X$:  Encoding the hashed values of candidates\n",
    "\n",
    "$X$ has the shape $km \\times M$ where\n",
    "- $k$ is the number of bits in the Bloom filter\n",
    "- $m$ is the number of cohorts\n",
    "- $M$ is the number of candidate strings\n",
    "\n",
    "Each candidate string corresponds to one feature. Each feature has $h * m$ values set to $1$, all others are $0$. This makes $X$ a sparse matrix.\n",
    "\n",
    "Each data point corresponds to one bit in a cohort. A cell in that row is set to $1$ if that bit would be set by using the respective hashed candidate string.\n",
    "\n",
    "---\n",
    "\n",
    "To get the bits set in a Bloom filter, we import a function from Google's repo. This is important because we need to make sure to use the same hash function in client and server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from client.rappor import get_bloom_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function gives us the bits that are set when the candidate string is hashed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bloom_bits(\"test\", 4, h, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can finally create $X$ by creating the matrices of the individual cohorts, and then stacking them vertically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix = []\n",
    "\n",
    "for cohort in range(m):\n",
    "    rows = []\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        bits = np.zeros(k)\n",
    "        bits_set = get_bloom_bits(candidate, cohort, h, k)\n",
    "        bits[bits_set] = 1\n",
    "        rows.append(bits)\n",
    "        \n",
    "    for row in np.array(rows).T:\n",
    "        matrix.append(row)\n",
    "\n",
    "X = np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 300)\n"
     ]
    }
   ],
   "source": [
    "print X.shape\n",
    "assert(X.shape == (k * m, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give some intuition on what we're doing here: The design matrix $X$ contains information about what bits would be set by which candidates. By using the estimated counts of the individual bits, we then try to infer which candidate strings really appeared.\n",
    "\n",
    "**TODO**: Why do we use the estimated results, instead of the observed counts?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso: Eliminating some candidates\n",
    "\n",
    "In the RAPPOR paper, Google says they fit a Lasso model and only continue working with candidate strings that have non-zero coefficients. This acts as a preliminary filter to make the next model simpler.\n",
    "\n",
    "**TODO**: Understand how exactly this helps\n",
    "\n",
    "However, actually, Google only performs this Lasso regression if the number of candidates $M$ is larger than $0.8 * m * k$, as this means the system is close to being underdetermined.\n",
    "\n",
    "**TODO**: Understand why this exact condition exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 102.4 | Lasso should be performed: True\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.8 * m * k\n",
    "perform_lasso = M > cut_off\n",
    "\n",
    "print M, cut_off, \"| Lasso should be performed:\", perform_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we can't use $y$ here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.,  0.,  0., -0.,  0., -0.,  0., -0., -0.,  0., -0.,  0., -0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,  0., -0.,\n",
       "       -0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0., -0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "       -0.,  0.,  0., -0., -0., -0.,  0., -0., -0., -0.,  0.,  0.,  0.,\n",
       "        0., -0.,  0.,  0., -0.,  0.,  0., -0., -0.,  0.,  0., -0., -0.,\n",
       "       -0., -0.,  0.,  0., -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0.,\n",
       "        0., -0., -0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0., -0., -0., -0.,  0.,  0., -0., -0.,  0., -0., -0., -0., -0.,\n",
       "       -0.,  0.,  0.,  0.,  0., -0., -0., -0.,  0.,  0.,  0., -0.,  0.,\n",
       "        0.,  0.,  0.,  0., -0.,  0., -0., -0.,  0.,  0., -0., -0.,  0.,\n",
       "       -0.,  0.,  0., -0., -0., -0., -0., -0., -0.,  0., -0., -0., -0.,\n",
       "        0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0.,  0., -0.,  0.,\n",
       "       -0.,  0., -0.,  0.,  0., -0., -0., -0.,  0.,  0., -0., -0., -0.,\n",
       "       -0.,  0., -0., -0.,  0.,  0., -0.,  0.,  0.,  0.,  0., -0.,  0.,\n",
       "        0., -0.,  0., -0.,  0.,  0.,  0.,  0., -0., -0.,  0.,  0., -0.,\n",
       "        0., -0., -0.,  0., -0., -0.,  0.,  0., -0., -0., -0.,  0.,  0.,\n",
       "       -0., -0., -0.,  0., -0., -0.,  0.,  0.,  0.,  0., -0.,  0., -0.,\n",
       "        0.,  0., -0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., -0.,  0., -0.,\n",
       "       -0.,  0.,  0.,  0., -0., -0., -0.,  0., -0., -0., -0., -0., -0.,\n",
       "       -0.,  0.,  0., -0.,  0., -0.,  0.,  0.,  0., -0.,  0.,  0., -0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso()\n",
    "clf.fit(X, y)\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, $Y$ is needed. Scaling the target values to larger values probably weakens the l1 penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   -0.        ,    -0.        ,    -0.        ,    -0.        ,\n",
       "        -158.48839076,    -0.        ,    -0.        ,    -0.        ,\n",
       "         883.22681299,    -0.        ,     0.        ,     0.        ,\n",
       "           0.        ,    56.76638169,  -161.49275698,  -212.66552663,\n",
       "           0.        ,  -468.52744405,    -0.        ,     0.        ,\n",
       "         710.43218257,    -0.        ,     0.        ,    -0.        ,\n",
       "           0.        ,   210.64604412,    87.83443809,  1485.82263726,\n",
       "        -969.60543633,   316.00682313,    72.38384404,    -0.        ,\n",
       "          -0.        ,   219.9638582 ,    -0.        ,     0.        ,\n",
       "           0.        ,  -233.13024682,    -0.        ,  -894.44822621,\n",
       "          -0.        ,     0.        ,   596.84358   ,    -0.        ,\n",
       "          27.28325282,    -0.        ,    -0.        ,   -32.28046818,\n",
       "          -0.        ,  -612.51464747,    -0.        ,  -237.15146499,\n",
       "        -660.1447172 ,  -399.55904226,   -71.18870424,     0.        ,\n",
       "          -0.        ,     0.        ,   709.74951519,     0.        ,\n",
       "          -0.        ,   638.44991447,     0.        ,    -0.        ,\n",
       "          -0.        ,  -711.91126147,   449.2912021 ,    -0.        ,\n",
       "         156.6627038 ,   723.17842324,    -0.        ,    -0.        ,\n",
       "        -193.77665941,  -353.6015827 ,  -555.60565462,  -220.36687432,\n",
       "           0.        ,  1235.90532534,    -0.        ,  -678.0794906 ,\n",
       "           8.553551  ,     0.        ,   166.68480594,  -469.3377889 ,\n",
       "           0.        ,    -0.        ,     0.        ,   -99.03944197,\n",
       "           0.        ,    -4.19869347,    -0.        ,   180.76295331,\n",
       "           0.        ,    68.71556974,  1557.1300565 ,   139.76236492,\n",
       "           0.        ,  -403.25449447,     0.        ,    -0.        ,\n",
       "          -0.        ,    -0.        ,  -318.62004737,     0.        ,\n",
       "           0.        ,  -187.52612096,     0.        ,    27.77659024,\n",
       "          -0.        ,     0.        ,   714.84110314,     0.        ,\n",
       "          -0.        ,     0.        ,  -110.60064825,     0.        ,\n",
       "          78.16939323,    55.28688232,    -0.        ,   391.87804876,\n",
       "           0.        ,     0.        ,     0.        ,   -68.38845319,\n",
       "        -197.39431032,  -466.38199234,    -0.        ,  -450.55433286,\n",
       "        -113.42198923,   292.28206532,   572.88954821,     0.        ,\n",
       "         539.53270752,    -0.        ,   663.05103928,  -489.38753499,\n",
       "         -37.03906032,    -0.        ,   178.74027823,     0.        ,\n",
       "          -0.        ,    -0.        ,   661.03376332,    -0.        ,\n",
       "          19.69895873,   537.08276772,  -546.69828822,     0.        ,\n",
       "          -0.        ,    -0.        ,  -309.05748237,     0.        ,\n",
       "        -173.55354154,   402.27948638,     0.        ,    -0.        ,\n",
       "           0.        ,   -46.71853415,  -212.47394027,  -478.76687976,\n",
       "        1111.81987813,     0.        ,     0.        ,     0.        ,\n",
       "       -1328.94046021,   712.8980479 , -1310.31564935,    -0.        ,\n",
       "         183.24714089,    -0.        ,    70.87807853,  -702.44624984,\n",
       "        -667.8606811 ,    -0.        ,    12.68201062,  -233.58854041,\n",
       "          -0.        ,   380.12344385,     0.        ,    -0.        ,\n",
       "         108.87162115,    -0.        ,     0.        ,    -0.        ,\n",
       "          -0.        ,  -592.32792061,    -1.35217996,   -12.29110545,\n",
       "           0.        ,    -0.        ,    -0.        ,    -1.80047948,\n",
       "          59.46166644,    -0.        ,     0.        ,    -0.        ,\n",
       "         -28.04040658,   176.50667525,   775.29440201,    -0.        ,\n",
       "         901.50949852,   993.74400215,     0.        ,   155.48594161,\n",
       "         174.13549916,   285.1204455 ,    -0.        ,  -321.74818941,\n",
       "        -474.89577782,   156.90690077,  -205.15760913,     0.        ,\n",
       "        -184.26556643,    -0.        ,    -0.        ,    62.0318056 ,\n",
       "          53.78672968,    -0.        ,   -75.46627989,    -0.        ,\n",
       "         290.03571128,     0.        ,    -0.        ,   283.5049224 ,\n",
       "          -0.        ,    -0.        ,     0.        ,    96.11160114,\n",
       "        -707.80945275,     0.        ,     0.        ,  -309.77639432,\n",
       "         -85.80390832, -1055.4752836 ,   213.56489272,     0.        ,\n",
       "        -290.87777119,    -0.        ,  -237.4764926 ,    -0.        ,\n",
       "           0.        ,    -0.        ,  -238.90914113,     0.        ,\n",
       "         789.07111399,     0.        ,    -0.        ,    -0.        ,\n",
       "          -0.        ,    -0.        ,     0.        ,    -0.        ,\n",
       "           0.        ,    -0.        ,     0.        ,  -312.4801272 ,\n",
       "         -86.07782398,     0.        ,    -0.        ,     0.        ,\n",
       "          -0.        ,   883.79488436,    -0.        ,  -240.66186777,\n",
       "       -1165.92153016,   466.58408477,    -0.        ,   231.36333889,\n",
       "           0.        ,  -149.52989824,   210.49242739,  -277.32859478,\n",
       "          -0.        ,    -0.        ,   718.91659327,     0.        ,\n",
       "        1379.19508206,     0.        ,    -0.        ,     0.        ,\n",
       "          -0.        ,  -173.67641279,   289.09284377,     0.        ,\n",
       "        -536.80057622,    -0.        ,     0.        ,  -200.54961036,\n",
       "          -0.        ,     0.        ,   298.44742221,  -261.74504561,\n",
       "          -0.        ,    -0.        ,    -0.        ,    -0.        ,\n",
       "           0.        ,     0.        ,  -528.15830508,     0.        ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Lasso()\n",
    "clf.fit(X, Y.reshape(128))\n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Figure out if this is really the proper way to do it. What we call `good_candidates` here doesn't have that much overlap with the results when not using Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8,  13,  20,  25,  26,  27,  29,  30,  33,  42,  44,  58,  61,\n",
       "        66,  68,  69,  77,  80,  82,  91,  93,  94,  95, 107, 110, 116,\n",
       "       117, 119, 129, 130, 132, 134, 138, 142, 144, 145, 153, 160, 165,\n",
       "       168, 170, 174, 177, 180, 192, 197, 198, 200, 201, 203, 204, 205,\n",
       "       209, 215, 216, 220, 223, 227, 234, 244, 261, 265, 267, 270, 274,\n",
       "       276, 282, 290])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_candidates = np.where(clf.coef_ > 0)[0]\n",
    "good_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(good_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v8',\n",
       " 'v13',\n",
       " 'v20',\n",
       " 'v25',\n",
       " 'v26',\n",
       " 'v27',\n",
       " 'v29',\n",
       " 'v30',\n",
       " 'v33',\n",
       " 'v42',\n",
       " 'v44',\n",
       " 'v58',\n",
       " 'v61',\n",
       " 'v66',\n",
       " 'v68',\n",
       " 'v69',\n",
       " 'v77',\n",
       " 'v80',\n",
       " 'v82',\n",
       " 'v91',\n",
       " 'v93',\n",
       " 'v94',\n",
       " 'v95',\n",
       " 'v107',\n",
       " 'v110',\n",
       " 'v116',\n",
       " 'v117',\n",
       " 'v119',\n",
       " 'v129',\n",
       " 'v130',\n",
       " 'v132',\n",
       " 'v134',\n",
       " 'v138',\n",
       " 'v142',\n",
       " 'v144',\n",
       " 'v145',\n",
       " 'v153',\n",
       " 'v160',\n",
       " 'v165',\n",
       " 'v168',\n",
       " 'v170',\n",
       " 'v174',\n",
       " 'v177',\n",
       " 'v180',\n",
       " 'v192',\n",
       " 'v197',\n",
       " 'v198',\n",
       " 'v200',\n",
       " 'v201',\n",
       " 'v203',\n",
       " 'v204',\n",
       " 'v205',\n",
       " 'v209',\n",
       " 'v215',\n",
       " 'v216',\n",
       " 'v220',\n",
       " 'v223',\n",
       " 'v227',\n",
       " 'v234',\n",
       " 'v244',\n",
       " 'v261',\n",
       " 'v265',\n",
       " 'v267',\n",
       " 'v270',\n",
       " 'v274',\n",
       " 'v276',\n",
       " 'v282',\n",
       " 'v290']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[candidates[i] for i in good_candidates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating standard deviations\n",
    "\n",
    "Because the estimated $y$ values are just expected values, it's useful to also take spread into account. We do this by calculating the standard deviations of our estimates.\n",
    "\n",
    "**TODO**: Understand how to derive this formula\n",
    "\n",
    "The formulas for this are not given in the paper, so the code below is directly adapted from the R code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _std_row(row, p01, p11, p2):\n",
    "    N = row[0]\n",
    "    v = row[1:]\n",
    "    \n",
    "    p_hats = (v - p01 * N) / (N * p2) # expectation of a true 1\n",
    "    p_hats = np.maximum(0, np.minimum(1, p_hats))\n",
    "    \n",
    "    r = p_hats * p11 + (1 - p_hats) * p01\n",
    "        \n",
    "    return N * r * (1 - r) / p2**2\n",
    "\n",
    "def calculate_variances(counts):\n",
    "    p01 = p * (1 - f / 2) + q * f / 2\n",
    "    p11 = q * (1 - f / 2) + p * f / 2\n",
    "    p2 = p11 - p01\n",
    "    \n",
    "    N = counts[0].as_matrix()\n",
    "    \n",
    "    return (np.sqrt(counts.apply(lambda row: _std_row(row, p01, p11, p2), axis=1).as_matrix().T) / N).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stds = calculate_variances(counts).reshape((k * m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will use the standard deviations to resample the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating the data matrix and target values\n",
    "\n",
    "I'm not entirely sure why this is needed, and it's not mentioned in the paper, but Google also normalizes $X$ and $y$ to new variables $A, b$.\n",
    "\n",
    "**TODO**: Understand why exactly this is needed. It seems like scaling normalization, but the $\\min$ term makes it a bit odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_normalization(stds):\n",
    "    w = 1 / stds\n",
    "    w_median = np.median(w[np.isfinite(w)])\n",
    "\n",
    "    if not np.isfinite(w_median):\n",
    "        w_median = 1\n",
    "\n",
    "    w = np.minimum(w, 2 * w_median)\n",
    "    w = w / w.mean()\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X, y, stds):\n",
    "    w = calculate_normalization(stds)\n",
    "    \n",
    "    # Multiplying to a digonal matrix is equivalent to multiplying each\n",
    "    # row of X with the corresponding cell on the diagonal\n",
    "    diag = np.diag(w)\n",
    "    A = diag.dot(X)\n",
    "    b = diag.dot(y)\n",
    "    \n",
    "    return A, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A, b = normalize(X, y, stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Linear Regression\n",
    "\n",
    "In the paper, Google says they perform a \"regular least-squares regression using the selected variables\" but this is not true.\n",
    "The input variables are changed a bit, as we saw before. Additionally, the linear regression [involves additional constraints](https://github.com/google/rappor/blob/master/analysis/R/alternative.R#L22-26) that are not described in the paper.\n",
    "\n",
    "The following constraints must hold for valid coefficients $x$:\n",
    "\n",
    "#### $x$ must be nonnegative\n",
    "\n",
    "The coefficients correspond to frequencies of how often the respective candidate was approximately reported by a client, so it makes sense for them to be nonnegative.\n",
    "\n",
    "A nonnegativity constraint is supported by many least squares implementations.\n",
    "\n",
    "####  $x$ sums up to a value smaller or equal to $1$\n",
    "\n",
    "$y$ contains frequencies, not counts. So it makes sense that $x$ also gives us frequencies.\n",
    "\n",
    "#### Standard deviation\n",
    "\n",
    "If we use $x$ to estimate the bit counts in the Bloom filter, this estimate may not exceed the $y$ estimates by more than 3 standard deviations. This is just a constraint to make sure that we don't end up with a solution $x$ that works super well for some estimates but not at all for others.\n",
    "\n",
    "This third constraint however makes things a bit more difficult. In general, sklearn and SciPy cannot deal with it automatically. Because of this, we have to fallback to a more low-level optimizer here that allows us to encode arbitrary constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize, nnls\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third constraint, we introduce a new variable $yy$ that gives a limit for how much predicted values may differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yy = np.minimum(1, np.maximum(y + 3 * stds, 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the generality of our optimizer, it's important to choose a good first guess $x_0$. We generate this guess by quickly training a model using the first two constraints, but without the third. This turns out to be the critical change that allows us to get the same results as the optimization library used in R by Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x0, _ = nnls(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is simply least squares, i.e. we want to minimize $||Ax - b||_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(x):\n",
    "    return norm(A.dot(x) - b, ord=2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we encode the third constraint, and run the minimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standard_deviation_constraint = {\n",
    "    \"type\": \"ineq\",\n",
    "    \"fun\": lambda x: max(X.dot(x) - yy)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.0122141 ,  0.        ,  0.00626256,\n",
       "        0.        ,  0.00142542,  0.        ,  0.00118144,  0.01139936,\n",
       "        0.        ,  0.        ,  0.00740197,  0.01199515,  0.        ,\n",
       "        0.00596828,  0.        ,  0.00262161,  0.0144633 ,  0.00573316,\n",
       "        0.00470197,  0.        ,  0.00426866,  0.00000127,  0.        ,\n",
       "        0.00490808,  0.01466152,  0.00050213,  0.00799106,  0.02208589,\n",
       "        0.01164468,  0.00193188,  0.00252793,  0.00642717,  0.00897546,\n",
       "        0.00290562,  0.00919938,  0.01372217,  0.01963937,  0.00316551,\n",
       "        0.01319987,  0.03596563,  0.01824495,  0.02301296,  0.00706265,\n",
       "        0.01584764,  0.02276268,  0.01837276,  0.02188966,  0.02858525,\n",
       "        0.01919453,  0.00000107,  0.01587027,  0.02812627,  0.02083231,\n",
       "        0.03074889,  0.02340054,  0.02276469,  0.01630729,  0.01411503,\n",
       "        0.02336635,  0.01287925,  0.01939591,  0.01017477,  0.00646371,\n",
       "        0.00262609,  0.01432434,  0.        ,  0.01763645,  0.00661325,\n",
       "        0.00980854,  0.0064082 ,  0.01003453,  0.00001303,  0.00721595,\n",
       "        0.00398304,  0.        ,  0.        ,  0.        ,  0.00157558,\n",
       "        0.00619285,  0.        ,  0.        ,  0.00299229,  0.00038783,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00342046,  0.00156002,  0.0111836 ,  0.        ,\n",
       "        0.        ,  0.00432315,  0.00000037,  0.        ,  0.        ,\n",
       "        0.00216267,  0.00453717,  0.00684802,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00000014,  0.        ,  0.        ,  0.00763406,\n",
       "        0.00000136,  0.00196461,  0.        ,  0.        ,  0.        ,\n",
       "        0.00110323,  0.        ,  0.00000007,  0.00000279,  0.00000441,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.0038203 ,  0.        ,  0.00205959,  0.00697193,  0.0016043 ,\n",
       "        0.0072901 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00000678,\n",
       "        0.        ,  0.        ,  0.00448948,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00283708,  0.0009781 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00452075,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00000052,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00911068,  0.        ,  0.        ,\n",
       "        0.        ,  0.00000055,  0.00560446,  0.0033649 ,  0.00000607,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00590981,  0.        ,\n",
       "        0.00000001,  0.        ,  0.        ,  0.0000166 ,  0.00165057,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00519947,\n",
       "        0.        ,  0.        ,  0.00002843,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00275257,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00020344,  0.        ,  0.        ,\n",
       "        0.        ,  0.00911402,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00303478,  0.        ,  0.        ,  0.00026786,\n",
       "        0.00046718,  0.        ,  0.00098684,  0.0026253 ,  0.        ,\n",
       "        0.        ,  0.00844802,  0.        ,  0.        ,  0.        ,\n",
       "        0.00649689,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00312561,  0.        ,  0.        ,  0.000548  ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00065485,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.001982  ,\n",
       "        0.00112118,  0.00586164,  0.00385061,  0.00720885,  0.        ,\n",
       "        0.00685183,  0.00985016,  0.00725521,  0.00524255,  0.        ,\n",
       "        0.00388025,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00162819,\n",
       "        0.        ,  0.        ,  0.01480398,  0.00012495,  0.00000083,\n",
       "        0.        ,  0.        ,  0.        ,  0.0017939 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = minimize(cost,\n",
    "         x0=x0,\n",
    "         method=\"SLSQP\",\n",
    "         constraints=standard_deviation_constraint,\n",
    "         bounds=zip(np.zeros(M), np.ones(M))\n",
    "        ).x\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends a bit on the dataset, but usually we get the exact same results as Google's RAPPOR here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can put it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y, stds):\n",
    "    A, b = normalize(X, y, stds)\n",
    "    yy = np.minimum(1, np.maximum(y + 3 * stds, 0.01))\n",
    "    \n",
    "    standard_deviation_constraint = {\n",
    "        \"type\": \"ineq\",\n",
    "        \"fun\": lambda x, X, yy: max(X.dot(x) - yy),\n",
    "        \"args\": (X, yy)\n",
    "    }\n",
    "    \n",
    "    x0, _ = nnls(A, b)\n",
    "    x = minimize(cost,\n",
    "         x0=x0,\n",
    "         method=\"TNC\", # TNC or SLSQP\n",
    "         constraints=standard_deviation_constraint,\n",
    "         bounds=zip(np.zeros(M), np.ones(M)),\n",
    "         options={ \"maxiter\": 1000 }\n",
    "        ).x\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TNC\n",
    "\n",
    "TNC finds the exact same optimum as R. However, this only works here because the best solution is independent of the third constraint in this case. Of course, generally that's not true, so we can't use TNC without ditching the third constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tnc_options = {\n",
    "    \"maxiter\": 100000000,\n",
    "    \"disp\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.01202243,  0.        ,  0.00631522,\n",
       "        0.        ,  0.00194803,  0.        ,  0.00088036,  0.01094045,\n",
       "        0.        ,  0.        ,  0.00796869,  0.01200593,  0.        ,\n",
       "        0.00336434,  0.        ,  0.00452192,  0.01597571,  0.00415869,\n",
       "        0.00338899,  0.        ,  0.0029721 ,  0.00018061,  0.        ,\n",
       "        0.00550691,  0.01441526,  0.        ,  0.00911609,  0.02064229,\n",
       "        0.01158552,  0.00328928,  0.00187851,  0.00719944,  0.00801538,\n",
       "        0.00171239,  0.00946284,  0.01388075,  0.02021571,  0.00311965,\n",
       "        0.01397979,  0.03639437,  0.01813637,  0.02429385,  0.0055791 ,\n",
       "        0.01542571,  0.02328737,  0.01957269,  0.02101293,  0.0279826 ,\n",
       "        0.01957256,  0.        ,  0.01446236,  0.02659779,  0.01923808,\n",
       "        0.02931485,  0.02273244,  0.02401595,  0.01858964,  0.01591082,\n",
       "        0.02278403,  0.01265723,  0.01866916,  0.00791199,  0.00686929,\n",
       "        0.00432133,  0.01594683,  0.        ,  0.01842568,  0.00539927,\n",
       "        0.00758905,  0.00629006,  0.00901242,  0.00142972,  0.00424307,\n",
       "        0.00414765,  0.        ,  0.        ,  0.        ,  0.00164263,\n",
       "        0.00562152,  0.        ,  0.        ,  0.00396218,  0.00142826,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00429447,  0.00155856,  0.01095106,  0.        ,\n",
       "        0.        ,  0.00364492,  0.        ,  0.        ,  0.        ,\n",
       "        0.00244718,  0.00576646,  0.00666494,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.01024305,\n",
       "        0.        ,  0.00177493,  0.        ,  0.        ,  0.        ,\n",
       "        0.00124704,  0.        ,  0.        ,  0.        ,  0.00011295,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.00405868,  0.        ,  0.00333738,  0.00882661,  0.00121996,\n",
       "        0.00529446,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00477675,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00201027,  0.00249629,  0.        ,\n",
       "        0.        ,  0.        ,  0.00469717,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00850675,  0.        ,  0.        ,\n",
       "        0.        ,  0.00002642,  0.00430252,  0.00464442,  0.00057906,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00521218,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00222239,  0.00235854,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00471061,\n",
       "        0.        ,  0.        ,  0.00368763,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00337751,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.00039037,  0.        ,  0.        ,\n",
       "        0.        ,  0.00840038,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00269199,  0.        ,  0.        ,  0.00006515,\n",
       "        0.00091891,  0.        ,  0.00144451,  0.        ,  0.        ,\n",
       "        0.        ,  0.00892233,  0.        ,  0.        ,  0.        ,\n",
       "        0.00596557,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.00274807,  0.        ,  0.        ,  0.00152584,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00341056,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00237144,\n",
       "        0.        ,  0.00551102,  0.00371912,  0.00544793,  0.        ,\n",
       "        0.00575774,  0.00907124,  0.00485338,  0.00409656,  0.        ,\n",
       "        0.00434186,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.00452253,\n",
       "        0.        ,  0.        ,  0.01538477,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.00116983,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(cost,\n",
    "         x0=x0,\n",
    "         method=\"TNC\",\n",
    "         bounds=zip(np.zeros(M), np.ones(M)),\n",
    "         options=tnc_options\n",
    "        ).x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "A popular way to get more data is resampling. Here, we use the standard deviations to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resample(y, stds):\n",
    "    y_old = y\n",
    "    \n",
    "    deviation = np.array([normal(0, std) for std in stds])\n",
    "    y = y + deviation\n",
    "    \n",
    "    print sum(y_old - y)\n",
    "    \n",
    "    stds = stds * np.sqrt(2)\n",
    "        \n",
    "    return y, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/scipy/optimize/_minimize.py:397: RuntimeWarning: Method TNC cannot handle constraints.\n",
      "  RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0189488375296\n",
      "-0.0324099411293\n",
      "0.118262578516\n",
      "0.0397616137327\n"
     ]
    }
   ],
   "source": [
    "coefs = []\n",
    "\n",
    "for i in range(5):\n",
    "    if i > 0:\n",
    "        y_resampled, stds_resampled = resample(y, stds)\n",
    "    else:\n",
    "        y_resampled, stds_resampled = y, stds\n",
    "        \n",
    "    coefs.append(fit(X, y_resampled, stds_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.        ,  0.01202243,  0.        ,  0.00631522,\n",
       "         0.        ,  0.00194803,  0.        ,  0.00088036,  0.01094045,\n",
       "         0.        ,  0.        ,  0.00796869,  0.01200593,  0.        ,\n",
       "         0.00336434,  0.        ,  0.00452192,  0.01597571,  0.00415869,\n",
       "         0.00338899,  0.        ,  0.0029721 ,  0.00018061,  0.        ,\n",
       "         0.00550691,  0.01441526,  0.        ,  0.00911609,  0.02064229,\n",
       "         0.01158552,  0.00328928,  0.00187851,  0.00719944,  0.00801538,\n",
       "         0.00171239,  0.00946284,  0.01388075,  0.02021571,  0.00311965,\n",
       "         0.01397979,  0.03639437,  0.01813637,  0.02429385,  0.0055791 ,\n",
       "         0.01542571,  0.02328737,  0.01957269,  0.02101293,  0.0279826 ,\n",
       "         0.01957256,  0.        ,  0.01446236,  0.02659779,  0.01923808,\n",
       "         0.02931485,  0.02273244,  0.02401595,  0.01858964,  0.01591082,\n",
       "         0.02278403,  0.01265723,  0.01866916,  0.00791199,  0.00686929,\n",
       "         0.00432133,  0.01594683,  0.        ,  0.01842568,  0.00539927,\n",
       "         0.00758905,  0.00629006,  0.00901242,  0.00142972,  0.00424307,\n",
       "         0.00414765,  0.        ,  0.        ,  0.        ,  0.00164263,\n",
       "         0.00562152,  0.        ,  0.        ,  0.00396218,  0.00142826,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00429447,  0.00155856,  0.01095106,  0.        ,\n",
       "         0.        ,  0.00364492,  0.        ,  0.        ,  0.        ,\n",
       "         0.00244718,  0.00576646,  0.00666494,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01024305,\n",
       "         0.        ,  0.00177493,  0.        ,  0.        ,  0.        ,\n",
       "         0.00124704,  0.        ,  0.        ,  0.        ,  0.00011295,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00405868,  0.        ,  0.00333738,  0.00882661,  0.00121996,\n",
       "         0.00529446,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00477675,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00201027,  0.00249629,  0.        ,\n",
       "         0.        ,  0.        ,  0.00469717,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00850675,  0.        ,  0.        ,\n",
       "         0.        ,  0.00002642,  0.00430252,  0.00464442,  0.00057906,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00521218,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00222239,  0.00235854,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00471061,\n",
       "         0.        ,  0.        ,  0.00368763,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00337751,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00039037,  0.        ,  0.        ,\n",
       "         0.        ,  0.00840038,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00269199,  0.        ,  0.        ,  0.00006515,\n",
       "         0.00091891,  0.        ,  0.00144451,  0.        ,  0.        ,\n",
       "         0.        ,  0.00892233,  0.        ,  0.        ,  0.        ,\n",
       "         0.00596557,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00274807,  0.        ,  0.        ,  0.00152584,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00341056,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00237144,\n",
       "         0.        ,  0.00551102,  0.00371912,  0.00544793,  0.        ,\n",
       "         0.00575774,  0.00907124,  0.00485338,  0.00409656,  0.        ,\n",
       "         0.00434186,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00452253,\n",
       "         0.        ,  0.        ,  0.01538477,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00116983,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.00450494,  0.        ,  0.00140738,\n",
       "         0.        ,  0.00729947,  0.        ,  0.00015395,  0.01102407,\n",
       "         0.        ,  0.        ,  0.00956721,  0.01069289,  0.        ,\n",
       "         0.        ,  0.        ,  0.00403373,  0.01292528,  0.        ,\n",
       "         0.0000152 ,  0.        ,  0.00614947,  0.        ,  0.        ,\n",
       "         0.00294406,  0.01122729,  0.00401052,  0.01337842,  0.02319486,\n",
       "         0.00685165,  0.00735124,  0.00616659,  0.00467194,  0.00646741,\n",
       "         0.00663347,  0.00637086,  0.0079926 ,  0.01837708,  0.00821721,\n",
       "         0.01613067,  0.03804126,  0.01988664,  0.02450495,  0.0039881 ,\n",
       "         0.01601094,  0.0240244 ,  0.02731738,  0.01407879,  0.02381254,\n",
       "         0.01851757,  0.        ,  0.0186502 ,  0.02401488,  0.02168452,\n",
       "         0.02799287,  0.01462893,  0.02116999,  0.01256364,  0.02262791,\n",
       "         0.01769503,  0.00876172,  0.01887657,  0.0086811 ,  0.00276611,\n",
       "         0.        ,  0.01471022,  0.        ,  0.01500122,  0.00114903,\n",
       "         0.00475618,  0.00668297,  0.01177323,  0.00379479,  0.00231863,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00254268,  0.        ,  0.        ,  0.0044329 ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00140522,  0.00637558,  0.00665235,  0.01327235,  0.        ,\n",
       "         0.        ,  0.00532348,  0.        ,  0.        ,  0.        ,\n",
       "         0.00758668,  0.        ,  0.00681792,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00052411,  0.        ,  0.00734803,\n",
       "         0.00121449,  0.00242447,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.0006548 ,  0.00096987,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00537213,  0.00648053,  0.00206408,\n",
       "         0.0006003 ,  0.00239507,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00801667,\n",
       "         0.        ,  0.0004405 ,  0.00475525,  0.        ,  0.        ,\n",
       "         0.00070944,  0.        ,  0.00665167,  0.        ,  0.        ,\n",
       "         0.        ,  0.00497446,  0.01032081,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00281302,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00425259,  0.00203944,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00759398,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00019051,  0.        ,\n",
       "         0.        ,  0.        ,  0.00237818,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.0013394 ,  0.        ,  0.        ,  0.00975204,  0.        ,\n",
       "         0.        ,  0.        ,  0.00010267,  0.0037118 ,  0.00089246,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0065794 ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00055351,  0.00367745,  0.        ,  0.0016714 ,\n",
       "         0.        ,  0.00133832,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00936362,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00317458,\n",
       "         0.        ,  0.        ,  0.00066972,  0.        ,  0.        ,\n",
       "         0.        ,  0.01347956,  0.        ,  0.00136228,  0.        ,\n",
       "         0.00300782,  0.        ,  0.002169  ,  0.        ,  0.        ,\n",
       "         0.00312089,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00436429,\n",
       "         0.        ,  0.00145511,  0.        ,  0.        ,  0.0063143 ,\n",
       "         0.        ,  0.0101116 ,  0.00124717,  0.00240918,  0.        ,\n",
       "         0.00398138,  0.01864545,  0.00387198,  0.00362552,  0.        ,\n",
       "         0.00467704,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00244623,  0.        ,  0.        ,  0.        ,  0.00549468,\n",
       "         0.        ,  0.        ,  0.01542681,  0.        ,  0.00158166,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.00982556,  0.00079908,  0.00630223,\n",
       "         0.        ,  0.00298997,  0.        ,  0.        ,  0.01103737,\n",
       "         0.        ,  0.        ,  0.00400138,  0.00999593,  0.        ,\n",
       "         0.00437805,  0.        ,  0.00413703,  0.01541357,  0.00525139,\n",
       "         0.00485464,  0.        ,  0.00103908,  0.        ,  0.        ,\n",
       "         0.00741205,  0.01364221,  0.        ,  0.01059594,  0.02092299,\n",
       "         0.00961508,  0.005056  ,  0.        ,  0.00844785,  0.00574005,\n",
       "         0.00515002,  0.00631897,  0.02045485,  0.01839296,  0.00589149,\n",
       "         0.00946979,  0.03757776,  0.02173382,  0.02700063,  0.00789121,\n",
       "         0.01373456,  0.02213216,  0.01573821,  0.01954367,  0.02827399,\n",
       "         0.02525817,  0.        ,  0.01127345,  0.03053249,  0.01739382,\n",
       "         0.02959013,  0.02556397,  0.0237229 ,  0.01418883,  0.01234893,\n",
       "         0.02361749,  0.00827227,  0.02292597,  0.00689548,  0.0074731 ,\n",
       "         0.        ,  0.01333433,  0.        ,  0.02137702,  0.00477592,\n",
       "         0.00928029,  0.0091719 ,  0.00772645,  0.00186137,  0.00456262,\n",
       "         0.00565407,  0.        ,  0.00284417,  0.00018708,  0.00319443,\n",
       "         0.00566701,  0.        ,  0.00326161,  0.        ,  0.00216458,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00033884,  0.        ,  0.00561642,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00322956,  0.00835951,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00728256,\n",
       "         0.00587744,  0.00802272,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00228634,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00810321,  0.        ,\n",
       "         0.00441444,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00147534,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00127015,\n",
       "         0.        ,  0.        ,  0.00670408,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.0025308 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.01458159,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00322665,  0.00117717,  0.00558946,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00045259,  0.        ,  0.        ,  0.00846215,  0.        ,\n",
       "         0.00021306,  0.        ,  0.        ,  0.        ,  0.00251183,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00479399,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00948367,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00070963,\n",
       "         0.00336475,  0.        ,  0.00298327,  0.        ,  0.        ,\n",
       "         0.        ,  0.01240105,  0.        ,  0.        ,  0.        ,\n",
       "         0.00757306,  0.        ,  0.00140744,  0.        ,  0.        ,\n",
       "         0.        ,  0.00413439,  0.        ,  0.        ,  0.00158395,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00189771,\n",
       "         0.        ,  0.00174635,  0.        ,  0.        ,  0.00258054,\n",
       "         0.00344378,  0.00540606,  0.00419844,  0.00523161,  0.        ,\n",
       "         0.00417557,  0.00972142,  0.00450252,  0.00703171,  0.        ,\n",
       "         0.00817749,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00047991,  0.        ,  0.00185349,\n",
       "         0.        ,  0.        ,  0.01518523,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.01319052,  0.        ,  0.00590433,\n",
       "         0.        ,  0.00520446,  0.        ,  0.0018156 ,  0.0093978 ,\n",
       "         0.        ,  0.        ,  0.01063308,  0.01103885,  0.        ,\n",
       "         0.00226575,  0.        ,  0.0067348 ,  0.01532772,  0.00418342,\n",
       "         0.00259481,  0.        ,  0.00394433,  0.00145911,  0.        ,\n",
       "         0.00539774,  0.01273554,  0.        ,  0.00973723,  0.02117135,\n",
       "         0.01060264,  0.004774  ,  0.00118074,  0.00714743,  0.00680712,\n",
       "         0.00076432,  0.00878843,  0.01353993,  0.02089268,  0.00394969,\n",
       "         0.01678711,  0.0362745 ,  0.01870546,  0.02403909,  0.005643  ,\n",
       "         0.01511156,  0.02359868,  0.02054689,  0.02270705,  0.02843383,\n",
       "         0.01871597,  0.        ,  0.01466424,  0.02604305,  0.01820299,\n",
       "         0.0266084 ,  0.02003031,  0.02478668,  0.01936719,  0.01662998,\n",
       "         0.02108253,  0.01306676,  0.01871311,  0.00732131,  0.00763809,\n",
       "         0.00468717,  0.01708843,  0.        ,  0.01743498,  0.00355228,\n",
       "         0.00698804,  0.00622765,  0.00884688,  0.00173746,  0.00039867,\n",
       "         0.00579527,  0.        ,  0.        ,  0.        ,  0.00104003,\n",
       "         0.00638028,  0.        ,  0.        ,  0.0037214 ,  0.00116986,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00457545,  0.00047432,  0.01342287,  0.        ,\n",
       "         0.        ,  0.0029955 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00365594,  0.00700959,  0.00766075,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0102184 ,\n",
       "         0.        ,  0.0007326 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.0025212 ,  0.        ,  0.00017054,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00234195,  0.        ,  0.00670841,  0.00670984,  0.00114124,\n",
       "         0.00592803,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00173528,\n",
       "         0.        ,  0.        ,  0.00581499,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00326815,  0.00201721,  0.00115257,\n",
       "         0.        ,  0.        ,  0.00619706,  0.        ,  0.        ,\n",
       "         0.00060305,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00834239,  0.        ,  0.        ,\n",
       "         0.        ,  0.00042597,  0.00287712,  0.00400217,  0.00006516,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00225905,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.0024969 ,  0.00344393,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00440348,\n",
       "         0.        ,  0.        ,  0.0059024 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00281206,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00165092,  0.        ,  0.        ,\n",
       "         0.        ,  0.00986451,  0.        ,  0.00138712,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00099468,\n",
       "         0.00017389,  0.        ,  0.00156752,  0.        ,  0.        ,\n",
       "         0.        ,  0.00876384,  0.        ,  0.        ,  0.        ,\n",
       "         0.00428188,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00223215,  0.        ,  0.        ,  0.00218578,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00197403,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00390915,\n",
       "         0.        ,  0.00350188,  0.00390669,  0.0044318 ,  0.        ,\n",
       "         0.00431608,  0.00795906,  0.00375517,  0.0020418 ,  0.        ,\n",
       "         0.00356541,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0031668 ,\n",
       "         0.        ,  0.        ,  0.01581818,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00263528,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.01011614,  0.        ,  0.00300147,\n",
       "         0.        ,  0.00398272,  0.        ,  0.        ,  0.01004953,\n",
       "         0.        ,  0.        ,  0.01210102,  0.00766731,  0.        ,\n",
       "         0.00092105,  0.        ,  0.00629851,  0.01690653,  0.        ,\n",
       "         0.00473131,  0.        ,  0.00507264,  0.00231441,  0.        ,\n",
       "         0.00463075,  0.01102678,  0.001825  ,  0.0092461 ,  0.02207524,\n",
       "         0.0107765 ,  0.00595714,  0.00306654,  0.00711432,  0.01020128,\n",
       "         0.00514618,  0.01070001,  0.00965106,  0.02262577,  0.00611516,\n",
       "         0.01813563,  0.03594716,  0.02109225,  0.02195078,  0.0096835 ,\n",
       "         0.01226421,  0.02205699,  0.01986018,  0.02468235,  0.0272916 ,\n",
       "         0.01721485,  0.        ,  0.01749167,  0.0248242 ,  0.02199227,\n",
       "         0.02656436,  0.02039225,  0.02282366,  0.01208048,  0.01961292,\n",
       "         0.02348124,  0.00961578,  0.02023659,  0.00834996,  0.00279566,\n",
       "         0.        ,  0.01594775,  0.        ,  0.01740138,  0.00180916,\n",
       "         0.00724609,  0.00911421,  0.00639228,  0.00430047,  0.00346295,\n",
       "         0.00361453,  0.        ,  0.        ,  0.        ,  0.00296083,\n",
       "         0.00071153,  0.        ,  0.        ,  0.00671451,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00334082,  0.0027984 ,  0.01350684,  0.        ,\n",
       "         0.        ,  0.00566751,  0.        ,  0.        ,  0.        ,\n",
       "         0.00374044,  0.00104784,  0.0092249 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00659802,\n",
       "         0.        ,  0.00134995,  0.        ,  0.        ,  0.        ,\n",
       "         0.00138626,  0.        ,  0.00018688,  0.        ,  0.        ,\n",
       "         0.        ,  0.00021945,  0.        ,  0.        ,  0.        ,\n",
       "         0.00256608,  0.        ,  0.00637374,  0.00714926,  0.        ,\n",
       "         0.00543146,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.0074322 ,\n",
       "         0.        ,  0.        ,  0.00757444,  0.        ,  0.        ,\n",
       "         0.00086668,  0.        ,  0.00281579,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00772197,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.0020509 ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00073616,  0.        ,\n",
       "         0.        ,  0.00469626,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00838496,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00054125,  0.        ,  0.00155862,\n",
       "         0.        ,  0.        ,  0.00137559,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00179594,  0.        ,\n",
       "         0.00051828,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00229953,\n",
       "         0.        ,  0.        ,  0.00218992,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00132209,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.00297603,  0.        ,  0.        ,\n",
       "         0.        ,  0.0103675 ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00312623,  0.        ,\n",
       "         0.        ,  0.01201613,  0.        ,  0.        ,  0.        ,\n",
       "         0.00249346,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.00161036,  0.        ,  0.        ,  0.        ,  0.00109037,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00030432,\n",
       "         0.        ,  0.00368127,  0.        ,  0.        ,  0.00287313,\n",
       "         0.00201158,  0.00829512,  0.00391348,  0.00411012,  0.        ,\n",
       "         0.0091801 ,  0.01169438,  0.00623876,  0.00026089,  0.        ,\n",
       "         0.0048407 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.01446329,  0.00214455,  0.00064508,\n",
       "         0.        ,  0.        ,  0.        ,  0.00199424,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "The fundamental idea here is that the coefficients found by the linear regression are significant if they are stable across several resamplings.\n",
    "\n",
    "In the following, the mean and standard deviation of each coefficient is calculated. Then, coefficients with a low standard deviation (in relation to their mean) are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_total = N.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_mean = np.floor(np.mean(coefs, axis=0) * N_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_std = np.std(coefs, axis=0, ddof=1) * N_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Google's code, `cut_off_factor` is set to $2$. In our case, we might want to set it to a higher value since we have less variance because our optimizer doesn't work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cut_off_factor = 2\n",
    "reported = np.where(coef_mean > (cut_off_factor * coef_std + 1e-6))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69,)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reported.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_coefs = coef_mean[reported]\n",
    "mod_stds = coef_std[reported]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can print a list of the candidates that were found and the corresponding estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    41.  36847.]\n",
      " [    55.  28014.]\n",
      " [    49.  27158.]\n",
      " [    53.  26402.]\n",
      " [    43.  24357.]\n",
      " [    57.  23303.]\n",
      " [    46.  23019.]\n",
      " [    60.  21732.]\n",
      " [    29.  21601.]\n",
      " [    56.  20669.]\n",
      " [    47.  20607.]\n",
      " [    48.  20404.]\n",
      " [    38.  20100.]\n",
      " [    42.  19910.]\n",
      " [    62.  19884.]\n",
      " [    50.  19855.]\n",
      " [    54.  19702.]\n",
      " [    68.  17928.]\n",
      " [    59.  17426.]\n",
      " [    66.  15405.]\n",
      " [    58.  15357.]\n",
      " [    18.  15309.]\n",
      " [    52.  15308.]\n",
      " [   287.  15255.]\n",
      " [    40.  14900.]\n",
      " [    45.  14509.]\n",
      " [    37.  13103.]\n",
      " [    26.  12609.]\n",
      " [   271.  11418.]\n",
      " [    93.  11353.]\n",
      " [   241.  11116.]\n",
      " [     9.  10489.]\n",
      " [    61.  10474.]\n",
      " [    28.  10414.]\n",
      " [    13.  10280.]\n",
      " [     2.   9931.]\n",
      " [    30.   9886.]\n",
      " [   226.   9495.]\n",
      " [   177.   9481.]\n",
      " [    12.   8854.]\n",
      " [    72.   8750.]\n",
      " [   114.   8338.]\n",
      " [    36.   8328.]\n",
      " [    63.   7831.]\n",
      " [   102.   7745.]\n",
      " [    71.   7497.]\n",
      " [   133.   7453.]\n",
      " [    34.   7446.]\n",
      " [    70.   7171.]\n",
      " [   157.   7128.]\n",
      " [    33.   6916.]\n",
      " [   266.   6565.]\n",
      " [    44.   6556.]\n",
      " [    64.   5508.]\n",
      " [   198.   5496.]\n",
      " [   270.   5482.]\n",
      " [    39.   5458.]\n",
      " [    31.   5285.]\n",
      " [    25.   5178.]\n",
      " [    17.   5145.]\n",
      " [   275.   5120.]\n",
      " [   245.   4664.]\n",
      " [   272.   4644.]\n",
      " [     4.   4586.]\n",
      " [   147.   4584.]\n",
      " [   132.   4358.]\n",
      " [   135.   4333.]\n",
      " [   268.   4326.]\n",
      " [     6.   4284.]\n",
      " [    80.   4184.]\n",
      " [    35.   3881.]\n",
      " [    75.   3842.]\n",
      " [    22.   3835.]\n",
      " [    91.   3785.]\n",
      " [    83.   3766.]\n",
      " [   144.   3731.]\n",
      " [   264.   3609.]\n",
      " [   209.   3598.]\n",
      " [    96.   3526.]\n",
      " [   100.   3486.]\n",
      " [   273.   3411.]\n",
      " [   101.   3410.]\n",
      " [   267.   3396.]\n",
      " [    69.   3337.]\n",
      " [   217.   3196.]\n",
      " [    20.   3116.]\n",
      " [   284.   3007.]\n",
      " [    74.   2997.]\n",
      " [   152.   2949.]\n",
      " [   116.   2860.]\n",
      " [    19.   2718.]\n",
      " [    73.   2624.]\n",
      " [    32.   2458.]\n",
      " [   259.   2390.]\n",
      " [   212.   2355.]\n",
      " [    92.   2296.]\n",
      " [   182.   2189.]\n",
      " [    15.   2185.]\n",
      " [   183.   2002.]\n",
      " [   204.   1841.]\n",
      " [   251.   1822.]\n",
      " [    65.   1801.]\n",
      " [   130.   1793.]\n",
      " [    79.   1767.]\n",
      " [   203.   1686.]\n",
      " [   184.   1558.]\n",
      " [   115.   1418.]\n",
      " [   261.   1376.]\n",
      " [   170.   1356.]\n",
      " [   171.   1347.]\n",
      " [   237.   1333.]\n",
      " [   254.   1277.]\n",
      " [    27.   1167.]\n",
      " [   293.   1159.]\n",
      " [   265.   1091.]\n",
      " [   120.   1030.]\n",
      " [   222.   1003.]\n",
      " [   156.    994.]\n",
      " [   234.    988.]\n",
      " [   162.    972.]\n",
      " [    84.    952.]\n",
      " [   250.    946.]\n",
      " [   153.    902.]\n",
      " [   235.    891.]\n",
      " [   134.    885.]\n",
      " [    23.    790.]\n",
      " [   187.    750.]\n",
      " [   247.    715.]\n",
      " [    82.    652.]\n",
      " [   238.    625.]\n",
      " [     8.    569.]\n",
      " [    77.    568.]\n",
      " [   231.    538.]\n",
      " [   280.    489.]\n",
      " [   154.    484.]\n",
      " [   136.    479.]\n",
      " [   124.    479.]\n",
      " [   289.    445.]\n",
      " [   288.    428.]\n",
      " [   195.    358.]\n",
      " [   219.    334.]\n",
      " [   150.    315.]\n",
      " [    90.    281.]\n",
      " [   228.    277.]\n",
      " [   243.    272.]\n",
      " [   221.    267.]\n",
      " [   122.    202.]\n",
      " [   123.    193.]\n",
      " [     3.    159.]\n",
      " [   168.    147.]\n",
      " [   200.    146.]\n",
      " [   160.    120.]\n",
      " [   216.    110.]\n",
      " [   112.    104.]\n",
      " [   282.     95.]\n",
      " [   181.     90.]\n",
      " [   146.     88.]\n",
      " [   126.     43.]\n",
      " [    78.     37.]\n",
      " [   202.     20.]\n",
      " [    99.      0.]\n",
      " [    98.      0.]\n",
      " [    97.      0.]\n",
      " [   105.      0.]\n",
      " [   104.      0.]\n",
      " [   103.      0.]\n",
      " [   106.      0.]\n",
      " [   299.      0.]\n",
      " [    95.      0.]\n",
      " [    94.      0.]\n",
      " [     1.      0.]\n",
      " [     5.      0.]\n",
      " [     7.      0.]\n",
      " [    10.      0.]\n",
      " [    11.      0.]\n",
      " [    14.      0.]\n",
      " [    16.      0.]\n",
      " [    21.      0.]\n",
      " [    24.      0.]\n",
      " [    51.      0.]\n",
      " [   108.      0.]\n",
      " [    67.      0.]\n",
      " [    76.      0.]\n",
      " [    81.      0.]\n",
      " [    85.      0.]\n",
      " [    86.      0.]\n",
      " [    87.      0.]\n",
      " [    88.      0.]\n",
      " [    89.      0.]\n",
      " [   107.      0.]\n",
      " [   149.      0.]\n",
      " [   109.      0.]\n",
      " [   227.      0.]\n",
      " [   248.      0.]\n",
      " [   246.      0.]\n",
      " [   244.      0.]\n",
      " [   242.      0.]\n",
      " [   240.      0.]\n",
      " [   239.      0.]\n",
      " [   236.      0.]\n",
      " [   233.      0.]\n",
      " [   232.      0.]\n",
      " [   230.      0.]\n",
      " [   229.      0.]\n",
      " [   225.      0.]\n",
      " [   205.      0.]\n",
      " [   224.      0.]\n",
      " [   223.      0.]\n",
      " [   220.      0.]\n",
      " [   218.      0.]\n",
      " [   215.      0.]\n",
      " [   214.      0.]\n",
      " [   213.      0.]\n",
      " [   211.      0.]\n",
      " [   210.      0.]\n",
      " [   208.      0.]\n",
      " [   207.      0.]\n",
      " [   249.      0.]\n",
      " [   252.      0.]\n",
      " [   253.      0.]\n",
      " [   255.      0.]\n",
      " [   297.      0.]\n",
      " [   296.      0.]\n",
      " [   295.      0.]\n",
      " [   294.      0.]\n",
      " [   292.      0.]\n",
      " [   291.      0.]\n",
      " [   290.      0.]\n",
      " [   286.      0.]\n",
      " [   285.      0.]\n",
      " [   283.      0.]\n",
      " [   281.      0.]\n",
      " [   279.      0.]\n",
      " [   278.      0.]\n",
      " [   277.      0.]\n",
      " [   276.      0.]\n",
      " [   274.      0.]\n",
      " [   269.      0.]\n",
      " [   263.      0.]\n",
      " [   262.      0.]\n",
      " [   260.      0.]\n",
      " [   258.      0.]\n",
      " [   257.      0.]\n",
      " [   256.      0.]\n",
      " [   206.      0.]\n",
      " [   201.      0.]\n",
      " [   110.      0.]\n",
      " [   138.      0.]\n",
      " [   158.      0.]\n",
      " [   155.      0.]\n",
      " [   151.      0.]\n",
      " [   298.      0.]\n",
      " [   148.      0.]\n",
      " [   145.      0.]\n",
      " [   143.      0.]\n",
      " [   142.      0.]\n",
      " [   141.      0.]\n",
      " [   140.      0.]\n",
      " [   139.      0.]\n",
      " [   137.      0.]\n",
      " [   199.      0.]\n",
      " [   131.      0.]\n",
      " [   129.      0.]\n",
      " [   128.      0.]\n",
      " [   127.      0.]\n",
      " [   125.      0.]\n",
      " [   121.      0.]\n",
      " [   119.      0.]\n",
      " [   118.      0.]\n",
      " [   117.      0.]\n",
      " [   113.      0.]\n",
      " [   111.      0.]\n",
      " [   159.      0.]\n",
      " [   161.      0.]\n",
      " [   163.      0.]\n",
      " [   164.      0.]\n",
      " [   197.      0.]\n",
      " [   196.      0.]\n",
      " [   194.      0.]\n",
      " [   193.      0.]\n",
      " [   192.      0.]\n",
      " [   191.      0.]\n",
      " [   190.      0.]\n",
      " [   189.      0.]\n",
      " [   188.      0.]\n",
      " [   186.      0.]\n",
      " [   185.      0.]\n",
      " [   180.      0.]\n",
      " [   179.      0.]\n",
      " [   178.      0.]\n",
      " [   176.      0.]\n",
      " [   175.      0.]\n",
      " [   174.      0.]\n",
      " [   173.      0.]\n",
      " [   172.      0.]\n",
      " [   169.      0.]\n",
      " [   167.      0.]\n",
      " [   166.      0.]\n",
      " [   165.      0.]\n",
      " [     0.      0.]]\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(coef_mean)[::-1]\n",
    "print np.array(zip(indices, coef_mean[indices]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "For a visual comparison, it's helpful to plot the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_data = DataFrame.from_csv(base + \"1/case_true_values.csv\")\n",
    "counts = dict(Counter(true_data[\"value\"]))\n",
    "original = [counts['v%d' % i] for i in range(1, 101)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original += [0] * (M - len(original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 300 artists>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116522990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(M), original, width=1., color='orange', edgecolor='darkorange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the reported data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = np.zeros(M)\n",
    "total[reported] = mod_coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, we don't want to report on estimates that are very low:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#total[total < 6000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 300 artists>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nsKAs4RwJswAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11653afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(M), original, width=1., color='orange', edgecolor='darkorange')\n",
    "plt.bar(range(M), total, width=1., edgecolor='darkblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the quality of decoding\n",
    "\n",
    "It would be nice to have an automatic way of judging how good the decoding worked.\n",
    "\n",
    "A simple way to do this is to calculate the absolute distance between the actual and the reported distributions.\n",
    "Finally, this measure is divided by the number of candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(actual, reported):\n",
    "    return np.sum(np.abs(actual - reported)) / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2313.8266666666668"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(org, coef_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring collisions\n",
    "\n",
    "Some collisions are to be expected because of the Bloom filter. These are just some experiments for automatically calculating this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'[0, 0]': 3,\n",
       "         '[0, 1]': 6,\n",
       "         '[0, 2]': 7,\n",
       "         '[0, 3]': 5,\n",
       "         '[1, 0]': 9,\n",
       "         '[1, 1]': 6,\n",
       "         '[1, 2]': 7,\n",
       "         '[1, 3]': 6,\n",
       "         '[2, 0]': 2,\n",
       "         '[2, 1]': 6,\n",
       "         '[2, 2]': 10,\n",
       "         '[2, 3]': 9,\n",
       "         '[3, 0]': 3,\n",
       "         '[3, 1]': 6,\n",
       "         '[3, 2]': 4,\n",
       "         '[3, 3]': 11})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = Counter([str(get_bloom_bits(\"v%d\" % i , 0, h, k)) for i in range(100)])\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "counter = defaultdict(int)\n",
    "\n",
    "for i in range(100):\n",
    "    candidate = \"v%d\" % i\n",
    "    hashed = []\n",
    "    \n",
    "    for cohort in range(m):\n",
    "        hashed += get_bloom_bits(\"v%d\" % i , cohort, h, k)\n",
    "    \n",
    "    counter[str(hashed)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Statistical Inference â€“ Bonferroni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Google repo, they calculate some more statistical properties. However, these values are not used for further filtering, they're just saved into variables.\n",
    "\n",
    "**TODO**: We still have to figure out if we want to use techniques like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 52)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:, reported].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_inference(X, y, N, mod_coefs, mod_stds, alpha):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper, $\\alpha$ is set to $0.5$; in the code it's additionally divided by the total number of reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.05 / N_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perform_inference(\n",
    "    X[:, reported],\n",
    "    y,\n",
    "    N_total,\n",
    "    mod_coefs,\n",
    "    mod_stds,\n",
    "    alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F, ps = f_regression(X_good, y_good)\n",
    "# ps_corrected = multipletests(ps, method='bonferroni')\n",
    "# ps_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# F.sort()\n",
    "# F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
