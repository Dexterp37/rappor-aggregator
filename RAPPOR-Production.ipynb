{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RAPPOR\n",
    "\n",
    "[RAPPOR](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf) is an algorithm that lets us estimate statistics about a user population, while also preserving the privacy of individual users.\n",
    "It's split into two components: A part where data is collected in a privacy respecting way, and a part where the aggregated information is decoded using statistical techniques.\n",
    "\n",
    "This notebook provides a reimplementation of second component, i.e. the statistical analysis. After having collected enough user reports, this analysis can be performed to estimate how often certain values were reported by clients.\n",
    "\n",
    "If no dataset is available yet, or you first want to test how well the algorithm works, the notebook also provides a way to automatically generate one for you.\n",
    "\n",
    "After having performed the entire analysis, the results are presented at the bottom of the notebook, in the form of a [list](#Listed) and a [plot](#Visually). For each candidate string that was detected, we provide an estimated count for how often this value was reported. Keep in mind that it's only possible to detect values that were reported sufficiently often and that there's no way to know which user reported which value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALLOWED_HASH_FUNCTIONS = [\"md5\", \"sha256\"]\n",
    "ALLOWED_DISTRIBUTIONS = [\"normal\", \"exponential\", \"uniform\", \"zipf1\", \"zipf1.5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the settings for the RAPPOR algorithm itself. First, the Bloom filter that's used can be configured. The comments behind each variable show the variable name used in the original paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_bits = 128         # k\n",
    "num_hash_functions = 2 # h\n",
    "num_cohorts = 100      # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the probabilities for adding noise to the Bloom filter can be set:\n",
    "\n",
    "- $f$: Probability for randomly setting a bit in the permanent randomized response (PRR)\n",
    "- $p$: Probability of setting a bit to 1 in the instantenous randomized response (IRR) if it was 0 in the PRR\n",
    "- $q$: Probability of setting a bit to 1 in the instantenous randomized response (IRR) if it was 1 in the PRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = 0\n",
    "p = 0.65 \n",
    "q = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis will only report strings as detected if there is sufficient evidence. This can be configured using a statistical significance level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significance_level = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reported values can either be hashed using `md5` or `sha256`.\n",
    "In Google's repository, `md5` is used. For generated datasets, the choice shouldn't really matter. For custom datasets, it's important to choose the same hash function that was also used for the data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_function = ALLOWED_HASH_FUNCTIONS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the analysis is done, a table showing the strings with the highest estimates is displayed. You can configure how many strings this table shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_displayed_results = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either automatically let this notebook generate data, or load an existing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_users = 1000000\n",
    "num_candidates = 100\n",
    "distribution = ALLOWED_DISTRIBUTIONS[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Loading an existing dataset\n",
    "\n",
    "If you already have a dataset that you want to load, change this flag to `False`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reported_data` should then be a Python list that contains tuples.\n",
    "The first element of each tuple is a numpy array that contains the reported bits. All these arrays need to have length `num_bits`. The second element is an integer that describes which cohort the respective user is assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reported_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `num_bits = 4`, this list might look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "reported_data_example = [\n",
    "    (np.array([1, 0, 1, 0]), 4),\n",
    "    (np.array([0, 1, 1, 1]), 2)\n",
    "    # , â€¦\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to check for specific values, `candidates` should be a list of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "candidates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset also contains the true counts, `true_counts` can be a list of them for the given candidate strings. This list needs to have the same length as `candidates`, and the indices must be aligned correctly, i.e. `true_counts[i]` must provide the true counts for `candidates[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_counts = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset is automatically generated, `true_counts` is filled with the correct data and `candidates` defaults to all reported values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: The RAPPOR implementation is starting from here. Only touch this part if you know what you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not automatically generated, the variables above need to be set correctly. Here, we perform some basic sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not generate_data:\n",
    "    if len(candidates) == 0:\n",
    "        raise ValueError(\"If the dataset is not automatically generated, \"\n",
    "                         \"you need to supply a list of candidates\")\n",
    "        \n",
    "    if len(reported_data) == 0:\n",
    "        raise ValueError(\"If the dataset is not automatically generated, \"\n",
    "                         \"you need to load the collected data\")\n",
    "        \n",
    "    if len(true_counts) > 0 and len(true_counts) != len(candidates):\n",
    "        raise ValueError(\"If you provide a list of true counts, there needs \"\n",
    "                         \"to be information about every candidate string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hash_function not in ALLOWED_HASH_FUNCTIONS:\n",
    "    raise NotImplementedError(\"Unimplemented hash function %s\" % hash_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if distribution not in ALLOWED_DISTRIBUTIONS:\n",
    "    raise NotImplementedError(\"Unimplemented distribution %s\" % distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.addPyFile(\"client/rappor.py\")\n",
    "sc.addPyFile(\"client/hmac_drbg.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rappor import get_bloom_bits as get_bloom_bits_md5\n",
    "from hashlib import sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bloom_bits_sha256(value, cohort, num_hash_functions, num_bits):\n",
    "    bits = []\n",
    "    \n",
    "    for hi in range(num_hash_functions):\n",
    "        seed = str(cohort) + str(hi)\n",
    "        digest = sha256(seed + value).digest()\n",
    "\n",
    "        bit = ord(digest[-1]) % num_bits\n",
    "        bits.append(bit)\n",
    "\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_functions = {\n",
    "    \"sha256\": get_bloom_bits_sha256,\n",
    "    \"md5\": get_bloom_bits_md5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bloom_bits = hash_functions[hash_function]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation (Test-Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.stats import rv_discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_normal(num_users, num_candidates):\n",
    "    return np.floor(np.random.normal(num_candidates / 2, num_candidates / 6, size=(num_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_uniform(num_users, num_candidates):\n",
    "    return np.floor(np.random.uniform(0, num_candidates, size=(num_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_exponential(num_users, num_candidates):\n",
    "    return np.floor(np.random.exponential(scale=num_candidates/5,\n",
    "                                          size=(num_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_custom_zipf(s, num_users, num_candidates):\n",
    "    pdf = 1. / np.array(range(1, num_candidates))**float(s)\n",
    "    pdf = pdf / pdf.sum()\n",
    "    distribution = rv_discrete(name='zipf1', values=(range(len(pdf)), pdf))\n",
    "    return distribution.rvs(size=num_users)\n",
    "\n",
    "def sample_zipf(s):\n",
    "    return partial(sample_custom_zipf, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it doesn't happen often, the distributions above can generate values that are not between $0$ and $num_candidates$. In this case, we filter them out and resample new values until we have $num_users$ valid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_out_of_bounds(seq, lower, upper):\n",
    "    seq = seq[seq >= lower]\n",
    "    seq = seq[seq < upper]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(num_users, num_candidates, distribution=sample_normal):\n",
    "    data = distribution(num_users, num_candidates)\n",
    "    data = filter_out_of_bounds(data, 0, num_candidates)\n",
    "    \n",
    "    while len(data) < num_users:\n",
    "        additional_data = distribution(num_users - len(data), num_candidates)\n",
    "        additional_data = filter_out_of_bounds(additional_data, 0, num_candidates)\n",
    "        data = np.append(data, additional_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_candidates(num_candidates):\n",
    "    return [\"v%d\" % i for i in range(1, num_candidates + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if len(candidates) == 0:\n",
    "    candidates = generate_candidates(num_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_map = {\n",
    "    \"normal\": sample_normal,\n",
    "    \"exponential\": sample_exponential,\n",
    "    \"uniform\": sample_uniform,\n",
    "    \"zipf1\": sample_zipf(1),\n",
    "    \"zipf1.5\": sample_zipf(1.5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_distribution = distribution_map[distribution]\n",
    "indices = sample(num_users, num_candidates, distribution=used_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reported_values = [candidates[int(i)] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment to cohorts\n",
    "\n",
    "We can reuse the sampling functions we create earlier! Here, all users are assigned to cohorts uniformly randomly. The same logic is used in the shield study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = map(int, sample(num_users, num_cohorts, distribution=sample_uniform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating user reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bloom_filter((reported_value, cohort)):\n",
    "    set_bits = get_bloom_bits(reported_value, cohort, num_hash_functions, num_bits)\n",
    "    \n",
    "    bits = np.zeros(num_bits)\n",
    "    bits[set_bits] = 1\n",
    "    \n",
    "    return bits, cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual bits are flipped according to Bernoulli distributions with probabilities $f, p, q$.\n",
    "Because numpy doesn't have helpers for these, we use the equivalent binomial distributions with `num_users = 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bernoulli(p, size):\n",
    "    return np.random.binomial(n=1, p=p, size=(size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prr((bits, cohort)):\n",
    "    randomized_bits = np.where(bernoulli(f, num_bits))[0]\n",
    "    bits[randomized_bits] = bernoulli(0.5, len(randomized_bits))\n",
    "    return bits, cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_irr((bits, cohort)):\n",
    "    result = np.zeros(num_bits)\n",
    "    set_bits = np.where(bits == 1)[0]\n",
    "    unset_bits = np.where(bits == 0)[0]\n",
    "    \n",
    "    result[set_bits] = bernoulli(q, len(set_bits))\n",
    "    result[unset_bits] = bernoulli(p, len(unset_bits))\n",
    "    \n",
    "    return result, cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if generate_data:\n",
    "    rdd = sc.parallelize(zip(reported_values, cohorts))\n",
    "    rdd = rdd.map(build_bloom_filter).map(build_prr).map(build_irr)\n",
    "    reported_data = rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_data:\n",
    "    true_counts = np.zeros(num_candidates)\n",
    "    idx, counts = np.unique(indices, return_counts=True)\n",
    "    idx = map(int, idx)\n",
    "    true_counts[idx] = counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing\n",
    "\n",
    "Individual user reports are not very useful to us, instead we need to sum up how often each bit position was reported.\n",
    "\n",
    "`total_reports_per_cohort` is a vector containing the number of reports from the individual cohorts. `bit_counts` is a matrix\n",
    "where the entry `bit_counts[i, j]` tells us how often bit `j` was set in cohort `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_counts = np.zeros((num_cohorts, num_bits))\n",
    "total_reports_per_cohort = np.zeros(num_cohorts)\n",
    "\n",
    "for bits, cohort in reported_data:\n",
    "    bit_counts[cohort] += bits\n",
    "    total_reports_per_cohort[cohort] += 1\n",
    "    \n",
    "bit_counts = bit_counts.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target values `y `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_bloom_count(bit_counts, total_reports_per_cohort):\n",
    "    Y = bit_counts - ((p + 0.5 * f * q - 0.5 * f * p) * total_reports_per_cohort)\n",
    "    Y /= ((1 - f) * (q - p))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_target_values(bit_counts, total_reports_per_cohort):\n",
    "    Y = estimate_bloom_count(bit_counts, total_reports_per_cohort)\n",
    "    return (Y / total_reports_per_cohort).T.reshape(num_bits * num_cohorts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = get_target_values(bit_counts, total_reports_per_cohort)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data matrix `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features(candidates):\n",
    "    matrix = []\n",
    "\n",
    "    for cohort in range(num_cohorts):\n",
    "        rows = []\n",
    "\n",
    "        for candidate in candidates:\n",
    "            bits = np.zeros(num_bits)\n",
    "            bits_set = get_bloom_bits(candidate, cohort, num_hash_functions, num_bits)\n",
    "            bits[bits_set] = 1\n",
    "            rows.append(bits)\n",
    "\n",
    "        for row in np.array(rows).T:\n",
    "            matrix.append(row)\n",
    "\n",
    "    X = np.array(matrix)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_features(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import nnls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y):\n",
    "    x0, _ = nnls(X, y)\n",
    "    return x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "from numpy.linalg import inv, norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_significant_estimates(X, y, params, num_candidates, significance_level):\n",
    "    bonferroni_corrected_level = significance_level / num_candidates\n",
    "\n",
    "    predictions = X.dot(params)\n",
    "    num_datapoints, num_features = X.shape\n",
    "    MSE = norm(y - predictions, ord=2)**2 / (num_datapoints - num_features)\n",
    "\n",
    "    var = MSE * inv(X.T.dot(X)).diagonal()\n",
    "    sd = np.sqrt(var)\n",
    "    ts = params / sd\n",
    "\n",
    "    degrees_of_freedom = num_datapoints - 1\n",
    "    p_values = np.array([2 * (1 - t.cdf(np.abs(i), degrees_of_freedom)) for i in ts])\n",
    "\n",
    "    significant_i = np.where(p_values <= bonferroni_corrected_level)[0]\n",
    "    significant = params[significant_i]\n",
    "\n",
    "    analyzed = np.zeros(num_candidates)\n",
    "    analyzed[significant_i] = significant\n",
    "    estimates = analyzed * total_reports_per_cohort.sum()\n",
    "    \n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimates = get_significant_estimates(X, y, params, num_candidates, significance_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presenting the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_estimate_df(candidates, estimates, original):\n",
    "    indices = np.argsort(estimates)[::-1]\n",
    "    reported_candidates = [candidates[i] for i in indices]\n",
    "    reported_estimates = np.array(estimates[indices], dtype=np.int32)\n",
    "    \n",
    "    columns = [\"Candidate\", \"Estimated count\"]\n",
    "    \n",
    "    if len(original) == len(estimates):\n",
    "        reported_original = np.array(original[indices], dtype=np.int32)\n",
    "        data = np.array(zip(reported_candidates, reported_estimates, reported_original))\n",
    "        columns.append(\"Actual count\")\n",
    "    else:\n",
    "        data = np.array(zip(reported_candidates, reported_estimates))\n",
    "\n",
    "    df = DataFrame(data=data)\n",
    "    df.columns = columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_displayed_results = min(num_displayed_results, len(candidates))\n",
    "create_estimate_df(candidates, estimates, true_counts).head(num_displayed_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "matplotlib.rcParams.update({'font.size': 19})\n",
    "\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "if len(true_counts) == len(estimates):\n",
    "    original_bar = plt.bar(range(num_candidates), true_counts,\n",
    "                           width=1., color='orange', edgecolor='darkorange', alpha=0.6)\n",
    "    handles.append(original_bar)\n",
    "    labels.append(\"True\")\n",
    "    \n",
    "reported_bar = plt.bar(range(num_candidates), estimates,\n",
    "                       width=1., color='blue', edgecolor='darkblue', alpha=0.6)\n",
    "handles.append(reported_bar)\n",
    "labels.append(\"Estimated\")\n",
    "\n",
    "plt.title(\"RAPPOR results\")\n",
    "plt.legend(handles, labels, prop={'size': 18})\n",
    "plt.xlabel(\"Index of candidate string\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "1246px",
    "left": "0px",
    "right": "936px",
    "top": "132px",
    "width": "209px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
